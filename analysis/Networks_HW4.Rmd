---
title: "Networks_HW4"
author: "KiseokUchicago"
date: "2021-04-28"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=11, fig.height=9,
                      error=TRUE, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE)
```

## Homework4 (Randomizations and Nestedness)
## Coding assignment for ECEV 44500 Networks in Ecology and Evolution
Professor: **Mercedes Pascual**, **Sergio A. Alcala Corona** \
Student: **Kiseok Lee** 

```{r}
# libraries
library(igraph)
library(bipartite)
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(vegan)
```

## 1. Randomizations

As a general guideline, basic considerations when selecting a randomization procedure are:\

1) What is the null hypothesis? For example, randomizing a plant-pollinator matrix while maintaining only the density assumes that pollinators have no dietary restritions and that plants have not evolved to be specialized for specific pollinators. In other words, that the niche breadth of each species is unlimited in this system. This is obviously a very permissive model.\
2) Limitations such as computation time. \

In this lab we will see a few algorithms for the sake of introduction. Broadly speaking, in the network ecology literature null models are:\

1) equiprobable: assign the same probability to each potential interaction in the network.\
2) probabilistic: assign a probability to each potential interaction proportional to the number of observed interactions between partners.\
3) fixed: randomly shuffle the interactions while preserving the observed number of partners of each species.\

Now let’s shuffle a matrix:
```{r}
num_iterations <- 10
memmott1999_binary <- 1*(memmott1999>0) # Make the data binary
null_model <- vegan::nullmodel(memmott1999_binary, method = 'r00') # Select a null model for the data
shuffled_r00 <- simulate(null_model, nsim = num_iterations) # Shuffle
# Shuffling produces an array with 10 matrices, each of them is a shuffled matrix:
apply(shuffled_r00, 3, dim) 

```

Calculate the density of each of these matrices:
```{r}
connectance <- function(m){
  d <- sum(m>0)/(nrow(m)*ncol(m))
  return(d)
}
# Note how connectance is preserved:
apply(shuffled_r00, MARGIN = 3, connectance) # Apply the function to the array. Note the MARGIN argument

```

## Problem 1. What kind of model (equiprobable, probabilistic, fixed) is r00? What does it conserve? As such, what kinds of properties of the network does it ignore that you may want to consider in a null model? Why?

"r00": non-sequential algorithm for binary matrices that only preserves the number of presences (fill).\
This properties are conserved:\
3) Fixed: randomly shuffle the interactions while preserving the observed number of partners of each species.\
These two properties are not conserved and ignored. \
1) Equiprobable: assign the same probability to each potential interaction in the network.\
2) Probabilistic: assign a probability to each potential interaction proportional to the number of observed interactions between partners.\
Because we have made the interaction matrix into a binary matrix, we removed the weight information of each interaction. Each interaction is not assigned the same probability.

## Problem 2. Can you program the algorithm for r00 by yourself?
```{r}
null_r00 <- function(M){
  M[,] <- sample(M, replace = F) # shuffle (do not replace)
  # sum(M)
  return(M)
}

# null_r00(memmott1999_binary)
sum(null_r00(memmott1999_binary)) # success
```

A more conservative null hypothesis is to conserve either the row or colum marginal sums, or both (‘fixed’ null model). Algorithms that conserve both are very restrictive, and are commonly based on randomly selecting 2x2 sub-matrices and then swapping their cell entries (“swap” algorithms). However, swap algorithms suffer from several disadvantages in computational time and sampling limitations of the “null space” (discussed in the references below). A new algorithm that overcomes those limitations is the curveball algorithm (Strona et al. 2014). Note that this is a sequential algorithm in which matrices are generated based on previous ones. So for accuracy, the first generated matrices should be discarded. This is done using the burnin argument.

```{r}
num_iterations <- 10
null_model_curveball <- vegan::nullmodel(memmott1999_binary, method = 'curveball') # Select a null model
shuffled_curveball <- simulate(null_model_curveball, nsim = num_iterations, burnin = 1000) #Shuffle
# Calculate the density of each of these matrices
apply(shuffled_curveball, MARGIN = 3, connectance)
```

Did the algorithm match the row and column sums?
```{r}
all(apply(shuffled_curveball, MARGIN = 3, rowSums)==rowSums(memmott1999_binary)) # all: Are all values true?
```

Did it shuffle any interactions?
```{r}
for (i in 1:num_iterations){
  if(!identical(shuffled_curveball[,,i], memmott1999_binary)){ # If the shuffled is NOT identical to the original
    print(paste('Shuffled matrix',i,'is different than the original'))
  }
}
```

Using these randomizations, let’s test if the binary version of the memmot1999 data is “statistically significantly” modular. Here we only present the code, without evaluating it because it takes time… Try running first one iteration before working with several for question 3.

```{r}
web <- memmott1999_binary
mod_observed <- computeModules(web)
Q_obs <- mod_observed@likelihood # This is the value of the modularity function Q for the observed network.

# Now shuffle
num_iterations <- 100
null_model <- vegan::nullmodel(web, method = 'r00') # Only preserve matrix fill (density)
shuffled_r00 <- simulate(null_model, nsim = num_iterations)
# Calculate modularity of the shuffled networks. For each shuffled network store the results in a vector
Q_shuff <- NULL
for (i in 1:10){
  mod <- computeModules(shuffled_r00[,,i])
  Q_shuff <- c(Q_shuff, mod@likelihood)
} 
#Which proportion of the simulations have a Q value larger than the observed?
P_value <- sum(Q_obs < Q_shuff)/length(Q_shuff)

```

## Problem 3. Pick another data set from the bipartite package and test if it is significantly modular compared to two null models. 

- Don’t forget to transform it to its binary version! (Or, alternatively, use null models for quantitative networks). One nice way to illustrate your result is to plot the distribution of the quantity of interest, here modularity Q, under the null model (i.e. for the randomizations), and to show there with an arrow or a different marker of your choice where the observation falls. \

- This plot shows clearly if the observation is unlikely under the random assumption, as the observation in that case should fall in the tail of the distribution. The P-value computed above is the area under the curve (of the probability density function) to the right of the observation. Try to produce a plot of this kind and briefly describe your finding. \

```{r}
# (1) Calculate observed Q
data(olesen2002flores) 
olesen2002flores_binary <- 1 * (olesen2002flores > 0) # Make the data binary (unweighted)
mod_olsen <- computeModules(olesen2002flores_binary)
Q_obs <- mod_olsen@likelihood

# (2) Calculate null Q
num_iterations <- 100
null_model_curveball <- vegan::nullmodel(olesen2002flores_binary, method = 'curveball') # Select a null model
shuffled_curveball <- simulate(null_model_curveball, nsim = num_iterations, burnin = 1000) #Shuffle
# Calculate the density of each of these matrices
apply(shuffled_curveball, MARGIN = 3, connectance)

shuffled_curveball[,,3]
# Calculate modularity of the shuffled networks. For each shuffled network store the results in a vector
Q_shuff <- NULL
for (i in 1:100){
  mod <- computeModules(shuffled_curveball[,,i])
  Q_shuff <- c(Q_shuff, mod@likelihood)
} 

# plot histogram
hist(Q_shuff, breaks=30, col="darkblue", border="slateblue", main="Histogram of Q of null model", xlab = 'modularity Q')
abline(v=Q_obs, col ="red")

```
The red line indicates the observed Q. Most of the values are smaller than observed Q.

```{r}
P_value <- sum(Q_obs < Q_shuff)/length(Q_shuff)
P_value
```
P value is the area of the probability density curve to the right of the Q observe.

## Problem 4. There is no better way to understand the mechanics of randomization algorithms than to program them yourself! 
Try to program the probabilistic model from [6]: “The probability of each cell being occupied is the average of the probabilities of occupancy of its row and column.” This means that the probability of drawing an interaction is proportional to the degree of both the lower-level and the higher-level species. Try to program this algorithm. 

```{r}
A <- olesen2002flores_binary
dim(A)
randomize6 <- function(A){
  # probability of edge matrix for each element A(i,j)
  prob_M <- matrix(rep(0,dim(A)[1]*dim(A)[2]), dim(A)[1],dim(A)[2])
  for (i in 1:dim(A)[1]){
    for(j in 1:dim(A)[2]){
      prob_row = sum(A[i,])/length((A[i,]))
      prob_col = sum(A[,j])/length((A[,j]))
      prob_M[i,j] = (prob_row + prob_col) / 2
    }
  }
  
  # get random number from uniform distribution [0,1]
  rand_M <- matrix(runif(dim(A)[1]*dim(A)[2]), dim(A)[1], dim(A)[2])
  
  # final randomized matrix
  randomized_matrix <- ifelse(rand_M < prob_M, 1, 0)
  colnames(randomized_matrix) <- colnames(A)
  rownames(randomized_matrix) <- rownames(A)
  
  return(randomized_matrix)
}

randomize6(A)
```

## 2. Recommended workflow



