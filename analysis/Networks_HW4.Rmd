---
title: "Networks_HW4"
author: "KiseokUchicago"
date: "2021-04-28"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=11, fig.height=9,
                      error=TRUE, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE)
```

## Homework4 (Randomizations and Nestedness)
## Coding assignment for ECEV 44500 Networks in Ecology and Evolution
Professor: **Mercedes Pascual**, **Sergio A. Alcala Corona** \
Student: **Kiseok Lee** 

```{r}
# libraries
library(igraph)
library(bipartite)
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(vegan)
```

## 1. Randomizations

As a general guideline, basic considerations when selecting a randomization procedure are:\

1) What is the null hypothesis? For example, randomizing a plant-pollinator matrix while maintaining only the density assumes that pollinators have no dietary restritions and that plants have not evolved to be specialized for specific pollinators. In other words, that the niche breadth of each species is unlimited in this system. This is obviously a very permissive model.\
2) Limitations such as computation time. \

In this lab we will see a few algorithms for the sake of introduction. Broadly speaking, in the network ecology literature null models are:\

1) equiprobable: assign the same probability to each potential interaction in the network.\
2) probabilistic: assign a probability to each potential interaction proportional to the number of observed interactions between partners.\
3) fixed: randomly shuffle the interactions while preserving the observed number of partners of each species.\

Now let’s shuffle a matrix:
```{r}
data("memmott1999")

num_iterations <- 10
memmott1999_binary <- 1*(memmott1999>0) # Make the data binary
null_model <- vegan::nullmodel(memmott1999_binary, method = 'r00') # Select a null model for the data
shuffled_r00 <- simulate(null_model, nsim = num_iterations) # Shuffle
# Shuffling produces an array with 10 matrices, each of them is a shuffled matrix:
apply(shuffled_r00, 3, dim) 

```

Calculate the density of each of these matrices:
```{r}
connectance <- function(m){
  d <- sum(m>0)/(nrow(m)*ncol(m))
  return(d)
}
# Note how connectance is preserved:
apply(shuffled_r00, MARGIN = 3, connectance) # Apply the function to the array. Note the MARGIN argument

```

## Problem 1. What kind of model (equiprobable, probabilistic, fixed) is r00? What does it conserve? As such, what kinds of properties of the network does it ignore that you may want to consider in a null model? Why?

"r00": non-sequential algorithm for binary matrices that only preserves the number of presences (fill).\
These 2 properties are conserved:\
1) Equiprobable: assign the same probability to each potential interaction in the network.\
3) Fixed: randomly shuffle the interactions while preserving the observed number of partners of each species.\
This property is not conserved and ignored. \
2) Probabilistic: assign a probability to each potential interaction proportional to the number of observed interactions between partners.\
Because we have made the interaction matrix into a binary matrix, we removed the weight information of each interaction. Each interaction is not assigned the same probability.

## Problem 2. Can you program the algorithm for r00 by yourself?
```{r}
null_r00 <- function(M){
  M[,] <- sample(M, replace = F) # shuffle (do not replace)
  # sum(M)
  return(M)
}

# null_r00(memmott1999_binary)
sum(null_r00(memmott1999_binary)) # success
```

A more conservative null hypothesis is to conserve either the row or colum marginal sums, or both (‘fixed’ null model). Algorithms that conserve both are very restrictive, and are commonly based on randomly selecting 2x2 sub-matrices and then swapping their cell entries (“swap” algorithms). However, swap algorithms suffer from several disadvantages in computational time and sampling limitations of the “null space” (discussed in the references below). A new algorithm that overcomes those limitations is the curveball algorithm (Strona et al. 2014). Note that this is a sequential algorithm in which matrices are generated based on previous ones. So for accuracy, the first generated matrices should be discarded. This is done using the burnin argument.

```{r}
num_iterations <- 10
null_model_curveball <- vegan::nullmodel(memmott1999_binary, method = 'curveball') # Select a null model
shuffled_curveball <- simulate(null_model_curveball, nsim = num_iterations, burnin = 1000) #Shuffle
# Calculate the density of each of these matrices
apply(shuffled_curveball, MARGIN = 3, connectance)
```

Did the algorithm match the row and column sums?
```{r}
all(apply(shuffled_curveball, MARGIN = 3, rowSums)==rowSums(memmott1999_binary)) # all: Are all values true?
```

Did it shuffle any interactions?
```{r}
for (i in 1:num_iterations){
  if(!identical(shuffled_curveball[,,i], memmott1999_binary)){ # If the shuffled is NOT identical to the original
    print(paste('Shuffled matrix',i,'is different than the original'))
  }
}
```

Using these randomizations, let’s test if the binary version of the memmot1999 data is “statistically significantly” modular. Here we only present the code, without evaluating it because it takes time… Try running first one iteration before working with several for question 3.

```{r}
web <- memmott1999_binary
mod_observed <- computeModules(web)
Q_obs <- mod_observed@likelihood # This is the value of the modularity function Q for the observed network.

# Now shuffle
num_iterations <- 100
null_model <- vegan::nullmodel(web, method = 'r00') # Only preserve matrix fill (density)
shuffled_r00 <- simulate(null_model, nsim = num_iterations)
# Calculate modularity of the shuffled networks. For each shuffled network store the results in a vector
Q_shuff <- NULL
for (i in 1:10){
  mod <- computeModules(shuffled_r00[,,i])
  Q_shuff <- c(Q_shuff, mod@likelihood)
} 
#Which proportion of the simulations have a Q value larger than the observed?
P_value <- sum(Q_obs < Q_shuff)/length(Q_shuff)

```

## Problem 3. Pick another data set from the bipartite package and test if it is significantly modular compared to two null models. 

- Don’t forget to transform it to its binary version! (Or, alternatively, use null models for quantitative networks). One nice way to illustrate your result is to plot the distribution of the quantity of interest, here modularity Q, under the null model (i.e. for the randomizations), and to show there with an arrow or a different marker of your choice where the observation falls. \

- This plot shows clearly if the observation is unlikely under the random assumption, as the observation in that case should fall in the tail of the distribution. The P-value computed above is the area under the curve (of the probability density function) to the right of the observation. Try to produce a plot of this kind and briefly describe your finding. \

```{r}
# (1) Calculate observed Q
data(olesen2002flores) 
olesen2002flores_binary <- 1 * (olesen2002flores > 0) # Make the data binary (unweighted)
mod_olsen <- computeModules(olesen2002flores_binary)
Q_obs <- mod_olsen@likelihood

# (2) Calculate null Q
num_iterations <- 100
null_model_curveball <- vegan::nullmodel(olesen2002flores_binary, method = 'curveball') # Select a null model
shuffled_curveball <- simulate(null_model_curveball, nsim = num_iterations, burnin = 1000) #Shuffle
# Calculate the density of each of these matrices
apply(shuffled_curveball, MARGIN = 3, connectance)

shuffled_curveball[,,3]
# Calculate modularity of the shuffled networks. For each shuffled network store the results in a vector
Q_shuff <- NULL
for (i in 1:100){
  mod <- computeModules(shuffled_curveball[,,i])
  Q_shuff <- c(Q_shuff, mod@likelihood)
} 

# plot histogram
hist(Q_shuff, breaks=30, col="darkblue", border="slateblue", main="Histogram of Q of null model", xlab = 'modularity Q')
abline(v=Q_obs, col ="red")

```
The red line indicates the observed Q. Most of the values are smaller than observed Q.

```{r}
P_value <- sum(Q_obs < Q_shuff)/length(Q_shuff)
P_value
```
P value is the area of the probability density curve to the right of the Q observe.

## Problem 4. There is no better way to understand the mechanics of randomization algorithms than to program them yourself! 
Try to program the probabilistic model from [6]: “The probability of each cell being occupied is the average of the probabilities of occupancy of its row and column.” This means that the probability of drawing an interaction is proportional to the degree of both the lower-level and the higher-level species. Try to program this algorithm. 

```{r}
A <- olesen2002flores_binary
dim(A)
randomize6 <- function(A){
  # probability of edge matrix for each element A(i,j)
  prob_M <- matrix(rep(0,dim(A)[1]*dim(A)[2]), dim(A)[1],dim(A)[2])
  for (i in 1:dim(A)[1]){
    for(j in 1:dim(A)[2]){
      prob_row = sum(A[i,])/length((A[i,]))
      prob_col = sum(A[,j])/length((A[,j]))
      prob_M[i,j] = (prob_row + prob_col) / 2
    }
  }
  
  # get random number from uniform distribution [0,1]
  rand_M <- matrix(runif(dim(A)[1]*dim(A)[2]), dim(A)[1], dim(A)[2])
  
  # final randomized matrix
  randomized_matrix <- ifelse(rand_M < prob_M, 1, 0)
  colnames(randomized_matrix) <- colnames(A)
  rownames(randomized_matrix) <- rownames(A)
  
  return(randomized_matrix)
}

randomize6(A)
```

## 2. Recommended workflow

The algorithms that calculate modularity and assign species to modules are not deterministic. These are search algorithms that may produce different results each time they are run. In addition, the value of Q can only be interpreted in relation to some null expectation. Therefore, a typical workflow when working with data would be:

(1) Calculate Q for the empirical network multiple (usually 100) times. \
(2) Select the run with the max(Q) and use this as the Qobs.\
(3) Decide on a randomization scheme and produce an ensemble of shuffled networks (usually >=1000).\
(4) Calculate the Qnull for each of these shuffled networks and create a distribution of these values.\
(5) Calculate the P value as the proportion of times Qnull > Qobs.\

## 3. Nestedness

There are several methods for calculting the nestedness of a matrix. NODF is the most widely used metric and we will focus on it. NODF is evaluated separately per rows and columns, and then these scores are combined to get the NODF of the whole network. You are encouraged to look at the rationale behind it and how it compares to other metrics [1,2], as described in the lecture. Nestedness has also been suggested for weighted networks [3,4], but we will work with binary networks.

Both vegan and bipartite have a function to calculate nestedness already implemented. The vegan package is more flexible. A basic example:

```{r}
data("memmott1999")
memmott1999_binary <- 1*(memmott1999>0)
nodf <- nestednodf(memmott1999_binary)
nodf

plot(nodf)
```

Using oecosimu in vegan, calculting nestedness of a web and comparing it to a null model is super-easy! First we will compare to an equiprobabe shuffling algorithm:
```{r}
nodf_eval_1 <- oecosimu(memmott1999_binary, nestednodf, "r00", nsimul = 100)
nodf_eval_1$statistic # This is the NODF of the observed network

names(nodf_eval_1$oecosimu) # look at the ?oecosimu help for details on what are these values.

nodf_eval_1 # The networks is significantly nested.
```

## Problem 5. Try comparing to a fixed shuffling algorithm like curveball. Are there differences in the results when using different null models? Why?

```{r}
nodf_eval_2 <- oecosimu(memmott1999_binary, nestednodf, "curveball", nsimul = 100)
nodf_eval_2$statistic # This is the NODF of the observed network

names(nodf_eval_2$oecosimu) # look at the ?oecosimu help for details on what are these values.

nodf_eval_2 # The networks is significantly nested.
```

It is not significant. Nestedness in not significantly differnt from the null model based on fixed shuffling algorithm like curveball. The major difference between r00 and curveball randomization algorithm is that r00 is non-sequential and curveball is sequential. 

## Problem 6. Nestedness has been shown to be correlated with network density and the number of species. That renders comparisons of nestedness scores across many networks statistically inappropriate. How can we compare values of nestedness across networks?

(1) First way to do it which is not recommended is to do a linear regression (nestedness ~ (density, dimension)) with a huge number of network datasets. Then, if it is linear, we can compare the nestedness of networks with different dimension/density by dividing the nestedness by the slope of regression to compensate for the increasing/decreasing effects by density or dimension. \

(2) Second way we can do is to use the nestedness definistion from this paper: Staniczenko, P. P., Kopp, J. C., & Allesina, S. (2013). The ghost of nestedness in ecological networks. Nature communications, 4(1), 1-6.\
With this spectral method, we will be able to derive the large dominant eigenvalues of adjacency matrix for each network and compare regardless of difference in network density or the number of species.\


## 4. Network robustness

The question of how networks with different structures respond to failure of (or attack on) nodes is fundamental across all of network science. For example, in information security, attack on a hub site vs a random site will result in fundamentally different cascades of information failure. These concepts are naturally adapted in ecology. For example, how will a food web collapse if the most connected species go extinct first, compared to when extinctions are random?

We will practice how to examine robustness with an existing implementation in R for bipartite networks in an upcoming lab. Here, for those of you who area curious and like to know about all this in advance to consider this subject for your class project, we direct you to look at functions second.extinct, slope.bipartite, and robustness. You can also read about the Attack Tolerance Curve in [5] and quantifying robustness in [6].

## 5. References

(1) Almeida-Neto M, Guimarães PR, Loyola RD, Ulrich W. A consistent metric for nestedness analysis in ecological systems: reconciling concept and measurement. Oikos. 2008;117: 1227–1239. \
(2) Ulrich W, Almeida-Neto M, Gotelli NJ. A consumer’s guide to nestedness analysis. Oikos. 2009;118: 3–17.\
(3) Almeida-Neto M, Ulrich W. A straightforward computational approach for measuring nestedness using quantitative matrices. Environmental Modelling & Software. 2011;26: 173–178.\
(4) Staniczenko PPA, Kopp JC, Allesina S. The ghost of nestedness in ecological networks. Nat Commun. 2013;4: 1391.\
(5) Memmott J, Waser NM, Price MV. Tolerance of pollination networks to species extinctions. Proc R Soc B. 2004;271: 2605–2611.\
(6) Burgos E, Ceva H, Perazzo RPJ, Devoto M, Medan D, Zimmermann M, et al. Why nestedness in mutualistic networks? J Theor Biol. 2007;249: 307–313.\



